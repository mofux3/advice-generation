{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading a Model from Huggingface and Running Inference\n",
    "This is a very basic and high-level overview of loading a model from Huggingface and inferencing it. \n",
    "To use it alongside an OpenAI-tuned model, query the API and replace the `prompt` with the return value. \n",
    "Ensure you have a valid Huggingface account, and have set up your token properly.\n",
    "\n",
    "# Huggingfaceからモデルをロードして推論を実行\n",
    "これは、Huggingfaceからモデルをロードし、推論を実行するための非常に基本的でハイレベルな概要です。\n",
    "OpenAIで調整されたモデルと一緒に使用するには、APIにクエリを送信し、`prompt`を返り値で置き換えます。\n",
    "有効なHuggingfaceアカウントを持っており、トークンを適切に設定していることを確認してください。"
   ],
   "id": "6d1e9e6b6d7a88d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T04:35:42.007576Z",
     "start_time": "2024-06-06T04:35:36.235827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install the transformers library if not already installed\n",
    "# 必要に応じてtransformersライブラリをインストール\n",
    "!pip install transformers\n",
    "!pip install torch"
   ],
   "id": "fcc5e36c83857938",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\beaum\\anaconda3\\envs\\meow\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import the necessary libraries\n",
    "# 必要なライブラリをインポート\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ],
   "id": "3c477ed1c6f76fd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define model name\n",
    "model_name = 'MODEL_REPO'"
   ],
   "id": "b1741f973f27b48b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the pre-trained model and tokenizer from Hugging Face\n",
    "# Hugging Faceから事前トレーニングされたモデルとトークナイザーをロード\n",
    "model_name = AutoModel.from_pretrained(model_name)\n",
    "tokenizer_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_name, model_max_tokens=2048, use_fast=False,\n",
    "    padding_side=\"right\", token='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'\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfloat16, device_map='cuda',\n",
    "    cache_dir='./workspace', token='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'\n",
    ")"
   ],
   "id": "bc859ea2635c0502"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define a sample prompt\n",
    "# サンプルプロンプトを定義\n",
    "prompt = \"SAMPLE PROMPT\""
   ],
   "id": "4d10bef830122188"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode the prompt into tokens\n",
    "# プロンプトをトークンにエンコード\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')"
   ],
   "id": "a36edda689c003d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate text using the model\n",
    "# モデルを使用してテキストを生成\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)"
   ],
   "id": "bcd42a903ee526d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Decode the generated tokens back to text\n",
    "# 生成されたトークンを再度テキストにデコード\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ],
   "id": "8cc6be14ef2538e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
