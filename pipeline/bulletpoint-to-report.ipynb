{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-20T06:16:43.900334Z",
     "start_time": "2024-06-20T06:16:43.882884Z"
    }
   },
   "source": "# Bullet points from OpenAI API to Full report utilizing RAG and LLaMa3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T06:18:57.212173Z",
     "start_time": "2024-06-20T06:18:56.559023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See if model is reachable\n",
    "\n",
    "import requests\n",
    "\n",
    "# Endpoint URL for the model\n",
    "endpoint = 'https://api.openai.com/v1/engines/g-ffT6XTGAx-road-condition-advisor-itemization/completions'\n",
    "\n",
    "# Make a GET request to the endpoint\n",
    "response = requests.get(endpoint)\n",
    "\n",
    "# Check the status code of the response\n",
    "if response.status_code == 200:\n",
    "    print(\"API endpoint is reachable.\")\n",
    "else:\n",
    "    print(f\"Failed to reach API endpoint. Status code: {response.status_code}\")\n"
   ],
   "id": "5ff48dcfb14396ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reach API endpoint. Status code: 401\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T06:16:44.417462Z",
     "start_time": "2024-06-20T06:16:43.905336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# OpenAI API key\n",
    "api_key = 'sk-proj-KtPDBOpf7ALzWOG4woQhT3BlbkFJH8udotmi3bsYDQg1Gnml'\n",
    "\n",
    "# URL Endpoint for the image to bullet-point model\n",
    "endpoint = 'https://api.openai.com/v1/engines/g-ffT6XTGAx-road-condition-advisor-itemization/completions'"
   ],
   "id": "d8efefd6775ac98c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T06:16:44.439480Z",
     "start_time": "2024-06-20T06:16:44.418438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_openai_model(image_url, text_input):\n",
    "    # Create request header\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "    # Create payload\n",
    "    payload = {\n",
    "        'prompt': f'Image: {image_url}\\nText: {text_input}',\n",
    "        'max_tokens': 150  # Adjust max_tokens as per your needs\n",
    "    }\n",
    "\n",
    "    # Send POSt request to the API\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "\n",
    "    # Error handling\n",
    "    if response.status_code == 200:\n",
    "        # Parse and return the response\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}, {response.text}\")\n",
    "        return None\n"
   ],
   "id": "afc7f1b5c5c23e18",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T06:16:45.161399Z",
     "start_time": "2024-06-20T06:16:44.499552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TEST case\n",
    "image_url = \"data/img/img1.JPG\"\n",
    "text_input = 'Describe what you see in this image.'\n",
    "\n",
    "# Query the OpenAI model\n",
    "result = query_openai_model(image_url, text_input)\n",
    "\n",
    "# Print the result\n",
    "if result:\n",
    "    print(json.dumps(result, indent=2))  # Pretty print JSON response\n",
    "    \n"
   ],
   "id": "77c47ce9d1deb3d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 404, {\n",
      "    \"error\": {\n",
      "        \"message\": \"The model `g-ffT6XTGAx-road-condition-advisor-itemization` does not exist or you do not have access to it.\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"model_not_found\"\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import streamlit for app dev\n",
    "# アプリ開発用にstreamlitをインポート\n",
    "import streamlit as st\n",
    "\n",
    "# Import transformer classes for generation\n",
    "# 生成用のtransformerクラスをインポート\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Import torch for datatype attributes \n",
    "# データ型属性用にtorchをインポート\n",
    "import torch\n",
    "# Import the prompt wrapper...but for llama index\n",
    "# プロンプトラッパーをインポート...ただしllama index用\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "# Import the llama index HF Wrapper\n",
    "# llama index HFラッパーをインポート\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "# Bring in embeddings wrapper\n",
    "# 埋め込みラッパーを持ち込む\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "# Bring in HF embeddings - need these to represent document chunks\n",
    "# HF埋め込みを持ち込む - ドキュメントチャンクを表現するために必要\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# Bring in stuff to change service context\n",
    "# サービスコンテキストを変更するためのものを持ち込む\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "# Import deps to load documents \n",
    "# ドキュメントをロードするための依存関係をインポート\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "from pathlib import Path"
   ],
   "id": "ec26878fc25df5ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define variable to hold llama2 weights naming \n",
    "# llama2の重み付け名を保持する変数を定義\n",
    "name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# Set auth token variable from hugging face \n",
    "# Hugging Faceから認証トークン変数を設定\n",
    "auth_token = \"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\""
   ],
   "id": "1c295bab55f43acd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_tokenizer_model():\n",
    "    # Create tokenizer\n",
    "    # トークナイザーを作成\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name, cache_dir='./model/', use_auth_token=auth_token)\n",
    "\n",
    "    # Create model\n",
    "    # モデルを作成\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        name, torch_dtype=torch.bfloat16, device_map='cuda',\n",
    "        cache_dir='./workspace', token=auth_token\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = get_tokenizer_model()"
   ],
   "id": "48c73e97ee72f87d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a system prompt \n",
    "# システムプロンプトを作成\n",
    "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as \n",
    "helpfully as possible, while being safe. Your answers should not include\n",
    "any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain \n",
    "why instead of answering something not correct. If you don't know the answer \n",
    "to a question, please don't share false information.\n",
    "\n",
    "Your goal is to create a proper advice report based on the provided bullet points. Use the provided data..<</SYS>>\n",
    "\"\"\"\n",
    "# Throw together the query wrapper\n",
    "# クエリラッパーをまとめる\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")"
   ],
   "id": "ad1b2fc4583a3ba3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a HF LLM using the llama index wrapper \n",
    "# llama indexラッパーを使用してHF LLMを作成\n",
    "llm = HuggingFaceLLM(context_window=4096,\n",
    "                     max_new_tokens=256,\n",
    "                     system_prompt=system_prompt,\n",
    "                     query_wrapper_prompt=query_wrapper_prompt,\n",
    "                     model=model,\n",
    "                     tokenizer=tokenizer)\n",
    "\n",
    "# Create and dl embeddings instance  \n",
    "# 埋め込みインスタンスを作成してダウンロード\n",
    "embeddings = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")"
   ],
   "id": "50d85084033076fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create new service context instance\n",
    "# 新しいサービスコンテキストインスタンスを作成\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embeddings\n",
    ")\n",
    "# Set service context\n",
    "# サービスコンテキストを設定\n",
    "set_global_service_context(service_context)"
   ],
   "id": "5f56f30c82d8e344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Download PDF Loader \n",
    "# PDFローダーをダウンロード\n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "# Create PDF Loader\n",
    "# PDFローダーを作成\n",
    "loader = PyMuPDFReader()\n",
    "# Load documents \n",
    "# ドキュメントをロード\n",
    "documents = loader.load(file_path=Path('documents'), metadata=True)\n",
    "\n",
    "# Create an index\n",
    "# インデックスを作成\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# I don't get this at all\n",
    "# これは全く理解できません\n",
    "query_engine = index.as_query_engine()"
   ],
   "id": "7f111fd0f870487c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"Base prompt\"\n",
    "\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(response)\n"
   ],
   "id": "2eb367898eaa7c2e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
